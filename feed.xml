<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://petrichor1998.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://petrichor1998.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-06T19:43:26+00:00</updated><id>https://petrichor1998.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Problog</title><link href="https://petrichor1998.github.io/blog/2023/Problog/" rel="alternate" type="text/html" title="Problog"/><published>2023-09-12T00:00:00+00:00</published><updated>2023-09-12T00:00:00+00:00</updated><id>https://petrichor1998.github.io/blog/2023/Problog</id><content type="html" xml:base="https://petrichor1998.github.io/blog/2023/Problog/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Problog <d-cite key="problog"></d-cite> is a probabilistic logic programming language. It is an extension of prolog with the addition of probabilistic facts and rules.</p> <h3 id="background">Background</h3> <p><strong>1) Monotone Disjunctive Normal Form (DNF):</strong> A DNF with no negated literals. <br/> <strong>2) Binary Decision Diagrams (BDDs) :</strong> A BDD is an acyclic directed graph with two terminal nodes 0 and 1. Each non-terminal node is labeled with a variable. The edges are labeled with 0 or 1. The BDD represents a boolean function. The BDD is constructed by recursively splitting the variables into two sets and then recursively constructing the BDD for each set. The BDD is constructed by splitting the variables in such a way that the number of nodes in the BDD is minimized.</p> <h3 id="example-program">Example Program</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0 :: likes(X,Y):- friendof(X,Y).
0.8 :: likes(X,Y):- friendof(X,Z), likes(Z,Y).
0.5 :: friendof(john,mary).
0.5 :: friendof(mary,pedro).
0.5 :: friendof(mary,tom).
0.5 :: friendof(pedro,tom).
</code></pre></div></div> <h3 id="inference">Inference</h3> <p>In Problog the success probability $P(q|T)$ of the query $q$ in a ProbLog program $T$ is computed. Where \(T = \{ p_1 : c_1, ..., p_n: c_n \}\). The ProbLog program $T$ defines a probability distribution over all logic programs \(L \subseteq L_T = \{c_1, ..., c_n\}\) as follows:</p> <p>\(\begin{align} P(L|T) &amp;= \prod_{c_i \in L} p_i \prod_{c_i \notin L} (1-p_i) \label{eq_1} \end{align}\) Above is a formal way of stating that the probability of a logic program $L$ is the product of the probabilities of the clauses in $L$ and the complement of the clauses not in $L$.</p> <p>The success probability of a query $q$ given a ProbLog program $T$ is defined as follows:</p> \[\begin{align} P(q|L) &amp;= \begin{cases} 1 &amp; \text{if } \exists \theta : L \vDash q\theta \\ 0 &amp; \text{otherwise } \end{cases} \label{eq_2} \\ P(q,L|T) &amp;= P(q|L)P(L|T) \label{eq_3} \\ P(q|T) &amp;= \sum_{M \subseteq L_T} P(q,M|T) \label{eq_4} \end{align}\] <p>As is apparent form eq. \(\eqref{eq_4}\) , the naive way of computing the probability is to enumerate all possible logic programs $M$ which is infeasible.<br/> Hence, the authors propose first computing the proofs of $q$ in $L_T$ which results in a monotone DNF formula and then compute the probability of this formula.</p> <p>For a given query, $q$ the SLD-tree is fist constructed and a DNF is constructed from the tree where each clause in disjunction is a successful proof as shown in Figure <a href="#fig_1">1</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"><a name="fig_1"></a> <figure> <picture> <img src="/assets/img/ProbLog/sld_tree_to_dnf.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="text-center"> Figure 1: SLD-tree to DNF </p> </div> <p>Now to calculate the $P((l_1 \land l_2 \land f_1 \land f_2 \land f_4) \lor (l_1 \land l_2 \land f_1 \land f_3))$ the authors construct a BDD which is a graphical representaion of a boolean function. Note, there are a lot of representations that can be used instead of a BDD. For instance SDD (Sentential Decision Diagrams). These were also used by Huang et. al in <d-cite key="scallop"></d-cite> and Manhaeve et. al in <d-cite key="deepproblog"></d-cite> for learning in a neurosymbolic setting.</p> <p>An approximate inference algorithm was also proposed by the authors because the exact inference might not be feasible in all applications.</p> <h2 id="conclusion">Conclusion</h2> <p>The major advantage of ProbLog over prolog is the functionality of adding probabilistic knowledge and hence working with uncertainity. Since the approximate inference algorithm is also proposed, one can find the probability of a query succeeding in a huge biological database wherein a possible connection between a certain gene and a disease can then be uncovered with a certain probability.</p> <h3 id="questions-that-need-to-be-addressed">Questions that need to be addressed</h3> <p><strong>Q) What is the SLD resolution?</strong> <br/> SLD resolution is a proof procedure for Horn clauses. It is a generalization of the resolution principle for propositional logic. It is a top-down procedure. This is done by attempting to match the goal with the heads of the clauses in the database. When a match is found, the system attempts to satisfy the conditions in the body of the clause. If the body conditions can be satisfied, the goal is replaced with subgoals representing the body conditions. A tree of proofs thus generated is called the SLD tree.</p> <p><strong>Q) What category of problems does problog aim to solve that prolog cannot?</strong><br/> ProbLog is used in domains where there is uncertainity. The facts and rules that have been found for example are uncertain. They are only true with a certain probability. For example, in a biological database, the fact that a certain gene is responsible for a certain disease is uncertain.</p> <p><strong>Q) What are the strengths and weaknesses of the paper (problog)?</strong> <br/> <em>Strengths</em> :<br/> 1) The inference is made more efficient because of the approximation algorithm of problog. <br/> 2) Most of the builtin predicates in prolog can be used directly.<br/> <em>Weakness</em> :<br/> 1) It is very hard to scale to big databases because the SLD-tree has to be calculated and then the intermediate representation such as BDD, OBDD, SDD etc. has to be constructed for the DNF and then the probability has to be calculated. This is a very expensive process.</p> <p><strong>Q) Can there be a probscasp?</strong> I personally think there can definitely be a probscasp and that would be interesting to see. The problem is of course the scaling it up. A critical questions to be asked is would probscasp solve anything that problog cannot? Maybe the answer would come to me sometime soon. Until then I will keep thinking about it.</p>]]></content><author><name>Parth Padalkar</name></author><category term="NeSy"/><summary type="html"><![CDATA[A probabilistic extension of prolog]]></summary></entry><entry><title type="html">The ERIC system and its applications</title><link href="https://petrichor1998.github.io/blog/2023/ERIC/" rel="alternate" type="text/html" title="The ERIC system and its applications"/><published>2023-06-02T00:00:00+00:00</published><updated>2023-06-02T00:00:00+00:00</updated><id>https://petrichor1998.github.io/blog/2023/ERIC</id><content type="html" xml:base="https://petrichor1998.github.io/blog/2023/ERIC/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>ERIC <d-cite key="eric"></d-cite> is a neurosymbolic framework that extracts rules from a CNN in the form of a decision tree. A CNN is trained on a dataset for image classification task and the last layer filters are used to extract rules that describe the knowledge learned by the CNN. A typical rule extracted by ERIC is of the form: $AB \land CD \land \neg EF \rightarrow bedroom$.</p> <h2 id="methodology">Methodology</h2> <p>The rule extraction and inference pipeline is shown below in Figure <a href="#fig_1">1</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"><a name="fig_1"></a> <figure> <picture> <img src="/assets/img/ERIC/inf_rule_extraction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="text-center"> Figure 1: ERIC rule extraction and inference pipeline </p> </div> <h3 id="quantization">Quantization</h3> <p>In the extraction pipeline the first step is to binarize the activations of the CNN filters. First, the norm of the last layer kernel activations are calculated. Then based on a threshold $\theta$, the outputs are binarized. The threshold is calculated as a weighted sum of the mean and standard deviation of norms of filter activations <d-cite key="ericAAAIworkshop"></d-cite>.</p> <h3 id="rule-extraction">Rule Extraction</h3> <p>The quantization step converts a subset of the training data into a 2D array of binary values, where a row represents an input image and a column represents a binarized filter activation. Now a simple decision tree learning algorithm like CART is used to extract a decision tree from this data.</p> <h3 id="visualization-and-labeling">Visualization and Labeling</h3> <p>The filters that appear in the rules are visualized and labeled by a human manually.</p> <h3 id="inference">Inference</h3> <p>The inference is done by quantizing the last layer filters activations of the test input and using the decision tree to predict the class label.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"><a name="fig_2"></a> <figure> <picture> <img src="/assets/img/ERIC/exp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="text-center"> Figure 2: The performance of ERIC on different datasets </p> </div> <h2 id="improvements-to-eric">Improvements to ERIC</h2> <p>The authors also introduced a novel approach for propagating the gradients is used called Elite backpropagation <d-cite key="elitebackprop"></d-cite>. Using Elite Backprop the class-wise activation sparsity is improved. EBP produces a clearer separation of kernel concepts and therefore also leads to more interpretable representations. The filters learnt are more crisp and precise as only a small number of filters are learnt to activate for each class. The algorithm for training using EBP is as follows:</p> <ol> <li>Train the CNN on the dataset.</li> <li>Quantize the last layer filter activations of the training data.</li> <li>Find top-k filters that have the highest activation for each class (<em>Elite filters</em>)</li> <li>Train the CNN again with a loss function that penalizes the activation of non-elite filters.</li> <li>Obtain the trained CNN with more precise filters after training.</li> </ol> <p>The idea is intuitive and it leads to better classification accuracy as well as better interpretability by virtue of sparse activation of filters. This seems to work better than a few other sparsity inducing methods that the authors compare against.</p> <h2 id="applications">Applications</h2> <p>Ngan et al. <d-cite key="ericmedical"></d-cite> show the application of ERIC for detecting covid 19 and pleural effusion from chest x-rays. They used the <a href="https://stanfordmlgroup.github.io/competitions/chexpert/">CheXpert</a> dataset. The authors find the top 10 images that most activate the filters that are selected and see what parts of the images they are activated by. This gives a kind of saliency map which they call <em>kernel fingerprints</em>. They cluster the kernel fingerprints in a 3D space and match them with their anatomic regions in the chest. This gives an idea about the biases that the network is learning. The sparse activation of filters leads to smaller rule-set size. They also do some evaluation on muting some selected filters and show that there is a great number of filters in the CNN that are redundant and can be removed without affecting the performance. This has been shown before <d-cite key="roleofindividualunits"></d-cite>.</p>]]></content><author><name>Parth Padalkar</name></author><category term="NeSy"/><summary type="html"><![CDATA[A neurosymbolic framework, ERIC, that extracts rules from a CNN in the form of a decision tree is proposed by Townsend et al. Its application in covid-19 and pleural effusion detection is discussed.]]></summary></entry><entry><title type="html">STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning</title><link href="https://petrichor1998.github.io/blog/2023/STaR/" rel="alternate" type="text/html" title="STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning"/><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>https://petrichor1998.github.io/blog/2023/STaR</id><content type="html" xml:base="https://petrichor1998.github.io/blog/2023/STaR/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Generating a step-by-step “chain-of-thought rationale” for solving complex reasoning problems has been shown to improve the Large Language Model’s (LLM’s) performance on the task <d-cite key="chain_of_thought"></d-cite>. Figure <a href="#fig:1">1</a> illustrates the approach of using a “chain-of-thought rationale” to solve an arithmetic word problem.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"><a name="fig:1"></a> <figure> <picture> <img src="/assets/img/chain_of_thought_prompting.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="text-center"> Figure 1: Illustration of the "chain-of-thought rationale" approach for solving an arithmetic word problem. </p> </div> <p>The paper “STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning” <d-cite key="star_bootstrap_reasoning"></d-cite> addresses the problem of the lack of availability of the rationale in the training data. The authors use GPT-J which is a open-source LLM with 6B parameters. In-Context learning does not give very good performace on complex reasoning tasks. Hence, fine-tuning the LLM on the training data is the only way (currently) to improve the performance of the LLm on the task. However, fine-tuning the LLM on the training data is not possible because the training data does not contain the rationale in most cases.</p> <h1 id="methodology">Methodology</h1> <p>The authors propose a bootstraping approach to generate rationales for the training data. The rationale is generated by the LLM itself as shown in Figure <a href="#fig:2">2</a>. The rationale is generated by the LLM by using the following steps:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"><a name="fig:2"></a> <figure> <picture> <img src="/assets/img/bootstrapping_in_star.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="text-center"> Figure 2: Illustration of the STaR algorithm. </p> </div> <ol> <li>The LLM is given a few prompts with the question, rationale and answer.</li> <li>For each example in the training data, the LLM is given the question asked to generate the rationale and the answer.</li> <li>If the answer is correct then the rationale is added to the corresponding example in the training data.</li> <li>If the answer is wrong then the answer is given to the LLM along with the question in the prompt to generate a new rationale and answer. (Rationalize)</li> <li>Filter the examples that the LLm generated correct rationales for based on the ground truth answers.</li> <li>Fine-tune the LLM on the filtered examples.</li> <li>Repeat steps 2-6 until the maximum number of iterations is reached.</li> </ol> <h1 id="results">Results</h1> <p>The authors show that this approach is great for generating rationales from limited training data on a vriety of tasks.</p> <h1 id="pros">Pros</h1> <ol> <li>The approach is effective in generating rationales for complex reasoning tasks.</li> </ol> <h1 id="cons">Cons</h1> <ol> <li>The LLM sometimes generates rationales that are based on why the other answers are incorrect rather than why the correct answer is correct. Which may be due to the rationalization.</li> <li>GPT-3 variants like davinci’s performance is not shown. I think it will be the best among all the LLMs.</li> </ol>]]></content><author><name>Parth Padalkar</name></author><category term="NLP"/><summary type="html"><![CDATA[Using LLMs for generating rationale for questions via bootstrapping]]></summary></entry><entry><title type="html">FOLD vs Decision Tree</title><link href="https://petrichor1998.github.io/blog/2023/FOLDvsDT/" rel="alternate" type="text/html" title="FOLD vs Decision Tree"/><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>https://petrichor1998.github.io/blog/2023/FOLDvsDT</id><content type="html" xml:base="https://petrichor1998.github.io/blog/2023/FOLDvsDT/"><![CDATA[<p>FOLDR++ <d-cite key="foldr++"></d-cite> and FOLDSE <d-cite key="foldse"></d-cite> both generate a decision list as their output. A decision list which is also called an ordered rule-set is a collection of individual classification rules that collectively for a classifier. The benefit of a decision list is that they are considered more interpretable than the decision tree because of their reduced complexity.Lakkaraju et. al. <d-cite key="decisionsets"></d-cite> introduce decision sets which they claim to be more interpretable than decision lists. They mention some limitations of decision lists such as the later rules in the list can only cover a narrow slice of the feature space. Moreover, the while interpreting the rules for a given decision it can be challenging to follow the path of the decision due to the need for checking every rule before the final rule is reached. They also discuss interpretability and explainability at great length in the paper.</p> <p>Here are the questions I encountered while reading about decision lists, decision trees and decision sets:</p> <p><strong>Q) Is there some optimal way of learning a decision tree?</strong> The authors Bertsimas et. al. <d-cite key="optdecisiontree"></d-cite> use Mixed integer Optimization (MIO) for the learning of a decision tree. They show their method of learning decision trees generates optimal trees and outperforms CART. Looking through the paper it is difficult to understand how they define “optimal” in this context.</p> <p><strong>Q) What are the ways in which optimality is defined?</strong> It could be finding the best hypothesis provably that gives highest accuracy on the test set, or it could be the size of the program. Optimal Classification Trees <d-cite key="optdecisiontree"></d-cite> uses the first definition of optimality. ILASP <d-cite key="ilasp"></d-cite> uses the second definition of optimality. Yu et. al. <d-cite key="optimaldecisionsetsandlistssat"></d-cite> define optimality as having minimum number of literals.</p> <p><strong>Q) Are there any theoretical foundations of a decision-list?</strong> Lot of work is available in this area. Heuristic based approaches include RIPPER and FOLDSE. According to different definitions of optimality, different articles are available. Yu et. al. <d-cite key="optimaldecisionsetsandlistssat"></d-cite> find optimal decision sets and decision lists in terms of the minimum number of literals. They use SAT solvers to find the most optimal decision sets/lists.</p> <p><strong>Q) How does FOLDSE compare against the decision-list generating algorithms?</strong> The only comparison availbale is the one with RIPPER but no details of the experiments are provided in terms of the hyperparameters used for RIPPER or FOLDSE both. Although the rule lists generated by FOLDSE are smaller, there is no explanation given as to why this is observed specially since both RIPPER and FOLDSE use a similar sequential covering algorithm for generating the rule set. A possible explanation could be the Magic GINI Impurity heuristic and what are its effect on RIPPER would be interesting to see. A comparison with other decision-list generating algorithms would be interesting.</p> <p><strong>Q) Are there any theoretical foundations of a decision-set?</strong> The authors of the paper Interpretable Decision Sets: A Joint Framework for Description and Prediction <d-cite key="decisionsets"></d-cite> present an algorithm for generating interpretable decision sets. They claim that as long as certain conditions are met the expressive power of a decision set is equivalent to that of a decision tree. They claim through human evaluation that “humans were three times more accurate given a decision set versus a decision list, and they used 74% fewer words and 71% less time to write their descriptions” They also consider “overlap” as a metric for interpretability. They define it as the number of data points that satisfy multiple rules. Note, FOLDSE does not have any overlap as the examples are removed as they are covered. They define an objective function based on interpretability and accuracy metrics which is non-negetive, non-normal, non-monotone and submodular. Then they use Smooth Local Search (SLS) algorithm to get the optimal desicion set. Because of these properties, the objective can be approximately optimized with theoretical guarantees.</p> <p><strong>Q) What optimality does ILASP talk about?</strong> The ILASP <d-cite key="ilasp"></d-cite> system searches for an optimal program. The authors define optimality as a program that has the <em>least</em> number of literals. They use CLINGO to find the optimal program. This is better than a greedy approach because it is guaranteed to find the optimal program. Hence maybe FOLDSE can be compared against ILASP.</p> <p><strong>Q) What type of algorithm does FOLDSE use in terms of the available literature in learning decision lists?</strong> FOLDSE uses sequential covering to generate the rule lists. It is an algorithm that is widely employed to generate rule lists and is also used in the popular rule learning algorithm RIPPER (Repeated Incremental Pruning to Produce Error Reduction) <d-cite key="interpretablemlbookmolnar"></d-cite>. Johannes Fürnkranz et. al. in their book Foundations of Rule Learning <d-cite key="foundationsofrulelearning"></d-cite> describe in detail various rule learning algorithms and their properties.</p> <p><strong>Q) What are the strengths of FOLDSE?</strong> FOLDSE is faster because of the prefix sum technique. The way the categorical and numerical values are compared could also be a contributing factor to the speed and size of the program generated.</p> <p><a href="# (Is there a way to get the FOLDSEM working better if the other classes are not eliminated?)">//</a>: # (There can be a final rule added that can serve as a “catch all” rule that the algorithm defaults to in case no other rule)</p>]]></content><author><name>Parth Padalkar</name></author><category term="XAI"/><summary type="html"><![CDATA[A comparison of the classification performance and the idea behind generating rules from the data.]]></summary></entry><entry><title type="html">NeurASP: Embracing Neural Networks into Answer Set Programming</title><link href="https://petrichor1998.github.io/blog/2023/neurasp/" rel="alternate" type="text/html" title="NeurASP: Embracing Neural Networks into Answer Set Programming"/><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>https://petrichor1998.github.io/blog/2023/neurasp</id><content type="html" xml:base="https://petrichor1998.github.io/blog/2023/neurasp/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>NeurASP <d-cite key="neurasp"></d-cite> is a framework that aims to combine the power of Answer Set Programming and neural networks. <a href="https://en.wikipedia.org/wiki/Answer_set_programming">Answer Set Progrramming</a> (ASP) is a form of declarative programming used to solve search, planning and combinatorial problems. It is also used for automating commonsense reasoning.</p> <p>In this paper there are mainly 2 ideas that the authors explore which are:</p> <ol> <li>Using pretrained neural network in symbolic computation to improve the neural network’s perception result</li> <li>Training a neural network from scratch using ASP rules as constraints.</li> </ol> <p>A neural atom <code class="language-plaintext highlighter-rouge">nn()</code> is introduced which represents a neural network in ASP. The idea is to use this atom to invoke a neural network whenever needed inside the ASP program. Some example code to using the neural atom is shown below:</p> <div class="language-prolog highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">img</span><span class="p">(</span><span class="ss">d1</span><span class="p">).</span> <span class="ss">img</span><span class="p">(</span><span class="ss">d2</span><span class="p">).</span>
<span class="ss">nn</span><span class="p">(</span><span class="ss">digit</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nv">X</span><span class="p">),</span> <span class="p">[</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">5</span><span class="p">,</span><span class="m">6</span><span class="p">,</span><span class="m">7</span><span class="p">,</span><span class="m">8</span><span class="p">,</span><span class="m">9</span><span class="p">])</span> <span class="p">:-</span> <span class="ss">img</span><span class="p">(</span><span class="nv">X</span><span class="p">).</span>
<span class="ss">addition</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span><span class="nv">B</span><span class="p">,</span><span class="nv">N</span><span class="p">)</span> <span class="p">:-</span> <span class="ss">digit1</span><span class="p">(</span><span class="nv">A</span><span class="p">)</span> <span class="o">=</span> <span class="nv">N1</span><span class="p">,</span> <span class="ss">digit1</span><span class="p">(</span><span class="nv">B</span><span class="p">)</span> <span class="o">=</span> <span class="nv">N2</span><span class="p">,</span> <span class="nv">N</span> <span class="o">=</span> <span class="nv">N1</span> <span class="o">+</span> <span class="nv">N2</span><span class="p">.</span>
</code></pre></div></div> <h2 id="learning">Learning</h2> <p>The <code class="language-plaintext highlighter-rouge">digit1()</code> is an atom that the <code class="language-plaintext highlighter-rouge">nn(digit(1, X), [0,1,2,3,4,5,6,7,8,9])</code> atom is distilled to. In the backend, it is broken down into 10 atoms, one for each digit. The rules above simply state that <code class="language-plaintext highlighter-rouge">d1</code> and <code class="language-plaintext highlighter-rouge">d2</code> are 2 images of digits, <code class="language-plaintext highlighter-rouge">digit</code> is the neural network used for the digit classification and the expected output can be any number <code class="language-plaintext highlighter-rouge">[0,1,2,3,4,5,6,7,8,9]</code> and the addition of the 2 digits is <code class="language-plaintext highlighter-rouge">A</code>. A probability is then associated with each atom <code class="language-plaintext highlighter-rouge">digit1(n)</code> where <code class="language-plaintext highlighter-rouge">n</code> is a digit, which is simply the output of the neural network. The above program solves the problem of adding 2 digits from images.</p> <p>Now, to find the sum of two numbers the query in ASP in <a href="https://potassco.org/clingo/">clingo</a> syntax is given as a constraint <code class="language-plaintext highlighter-rouge">:- addition(A,B,1)</code>. This query implies that the sum of the 2 digits should be 1. This is what the authors call the observation. Clingo then finds the stable models that satisfy the constraints which are the solution.</p> <p>The probability of these stable models can be calculated using the following formula: \(\begin{equation}\label{eq_1} P_{\Pi}(I) = \begin{cases} \frac{\prod_{(c = v)} P_{\Pi}(c = v)}{NumStableModels},&amp; \text{if } I \text{ is a stable model of } \Pi \\ 0, &amp; \text{otherwise}\\ \end{cases} \end{equation}\)</p> <p>in \eqref{eq_1}, $P_{\Pi}(I)$ is the probability of the stable model $I$ given the program $\Pi$. The probability of the observation is simply the sum of probabilities of all the stable models that satisfy the observation. Then the objective becomes to maximize the probability of the observation. Hence gradient ascent!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/learning_in_neurasp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Learning and gradient propogation in NeurASP </div> <p>In the above figure the red arrow indicates the gradient calculateion of the ASP program w.r.t the neural network outputs. This is calculated by using a formula given by the authors in the paper. Since the program is not continuous the gradient is difficult to calculate. The orange arrow indicates the calculateion of gradients of the outputs of the neural network w.r.t the network parameters. This is simply done using backpropogation.</p> <h2 id="inference">Inference</h2> <p>The inference procedure is straightforward wherein a trained neural network is used through the <code class="language-plaintext highlighter-rouge">nn()</code> atom in the ASP program to solve the problem.</p> <h2 id="pros">Pros</h2> <ol> <li>The impovement paper shows over DeepProblog <d-cite key="deepproblog"></d-cite> is notable. There is a significant improvement in terms of convergence time.</li> <li>Seemless integration and a strong proposition for combining neural networks and ASP in the training loop.</li> <li>ASP is more expressive than prolog so it could be seen as a slight advantage over DeepProblog which uses prolog like semantics in terms of knowledge representation.</li> </ol> <h2 id="cons">Cons</h2> <ol> <li>No theoretical proof to show that the given proposition for gradient calculation is correct and is optimal. (Atleast for me it seems like there is some exposition into the insight behind this is needed here.)</li> <li>SCALABILITY is a big issue. It is simply not scalable for bigger and more complex problems since ASP is used. ASP solvers like CLINGO ground the whole program and then find the stable models which simply becomes intractable.</li> </ol> <p>This framework falls under the Symbolic[Neuro] category of neurosymbolic systems according to the taxonomy given by Henry Kautz <d-cite key="NeSytaxonomy"></d-cite>. Here the neural network is being used as a part of the ASP semantics.</p>]]></content><author><name>Parth Padalkar</name></author><category term="NeSy"/><summary type="html"><![CDATA[A Neurosymbolic framework that combines Neural networks and Answer Set Programming]]></summary></entry></feed>